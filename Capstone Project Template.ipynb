{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering of the United States's Immigration Datasets.\n",
    "\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "For this Capstone project, I have been tasked to conduct a study on the Immigration Dataset of the united states with regards to World Temperature Data, U.S. City Demographic Data, and Airport Code Data. The objective here is to answer questions such as, what is the percentage of people that immigrate to the U.S from countries with cooler weather?, etc..\n",
    "\n",
    "The goal is to create ETL pipelines that extract each dataset from the source, apply several levels of transformation, then make it ready to be loaded into an analytics database (data warehouse) with a new consolidated data model that serves the analysis and study needs.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Library\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, date_add, desc, asc, col, when, to_date, lit\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql.types import DoubleType, StringType, IntegerType, FloatType, DateType, StructType, StructField\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope:\n",
    "The scope of the project is to create a data analytics warehouse(DWH) that is suitable for analysis purposes using Apache Spark. It includes extracting data from several sources, transforming each dataset to a specific form, then loading them into a new data model.\n",
    "\n",
    "This data warehouse will be used to analyze the U.S. immigration data regarding several perspectives, such as world tempreture, demographic data, and others. In the end, every step needed to create the new data model will be pipelined to make it reusable and consistent.\n",
    "\n",
    "#### Describe and Gather Data: \n",
    "The provided datasets for this project are briefly outlined as follows:\n",
    "\n",
    "**(a) I94 Immigration Data:** This data comes from the US National Tourism and Trade Office. This is where the data comes from:  https://travel.trade.gov/research/reports/i94/historical/2016.html. There's a sample file so you can take a look at the data in csv format before reading it all in. You do not have to use the entire dataset, just use what you need to accomplish the goal you set at the beginning of the project.\n",
    "\n",
    "**(b) World Temperature Data:** This dataset came from Kaggle. You can read more about it here: https://www.kaggle.com/datasets/berkeleyearth/climate-change-earth-surface-temperature-data.\n",
    "\n",
    "**(c) U.S. City Demographic Data:** This data comes from OpenSoft. You can read more about it here: https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/.\n",
    "\n",
    "**(d) Airport Code Table:** This is a simple table of airport codes and corresponding cities. It comes from here:https://datahub.io/core/airport-codes#data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Immigration dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()\n",
    "\n",
    "df_spark = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total count of the records\n",
    "df_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "#df_spark.write.parquet(\"sas_data\")\n",
    "#df_spark=spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Snapshot of the Immigration dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2027561</td>\n",
       "      <td>4084316.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20566.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>07202016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JL</td>\n",
       "      <td>5.658267e+10</td>\n",
       "      <td>00782</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2171295</td>\n",
       "      <td>4422636.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>MCA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>10222016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*GA</td>\n",
       "      <td>9.436200e+10</td>\n",
       "      <td>XBLNG</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>589494</td>\n",
       "      <td>1195600.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>OGG</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>07052016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LH</td>\n",
       "      <td>5.578047e+10</td>\n",
       "      <td>00464</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2631158</td>\n",
       "      <td>5291768.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20572.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>10272016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QR</td>\n",
       "      <td>9.478970e+10</td>\n",
       "      <td>00739</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032257</td>\n",
       "      <td>985523.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>CHM</td>\n",
       "      <td>20550.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>07042016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.232257e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  \\\n",
       "0     2027561  4084316.0  2016.0     4.0   209.0   209.0     HHW  20566.0   \n",
       "1     2171295  4422636.0  2016.0     4.0   582.0   582.0     MCA  20567.0   \n",
       "2      589494  1195600.0  2016.0     4.0   148.0   112.0     OGG  20551.0   \n",
       "3     2631158  5291768.0  2016.0     4.0   297.0   297.0     LOS  20572.0   \n",
       "4     3032257   985523.0  2016.0     4.0   111.0   111.0     CHM  20550.0   \n",
       "\n",
       "   i94mode i94addr    ...     entdepu  matflag  biryear   dtaddto  gender  \\\n",
       "0      1.0      HI    ...         NaN        M   1955.0  07202016       F   \n",
       "1      1.0      TX    ...         NaN        M   1990.0  10222016       M   \n",
       "2      1.0      FL    ...         NaN        M   1940.0  07052016       M   \n",
       "3      1.0      CA    ...         NaN        M   1991.0  10272016       M   \n",
       "4      3.0      NY    ...         NaN        M   1997.0  07042016       F   \n",
       "\n",
       "  insnum airline        admnum  fltno  visatype  \n",
       "0    NaN      JL  5.658267e+10  00782        WT  \n",
       "1    NaN     *GA  9.436200e+10  XBLNG        B2  \n",
       "2    NaN      LH  5.578047e+10  00464        WT  \n",
       "3    NaN      QR  9.478970e+10  00739        B2  \n",
       "4    NaN     NaN  4.232257e+10   LAND        WT  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the data here\n",
    "img_df_sample = pd.read_csv('immigration_data_sample.csv')\n",
    "img_df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The definition of the fields is included in the file **I94_SAS_Labels_Descriptions.SAS** We'll primarily be interested in the following fields:\n",
    "\n",
    "**(a) i94cit** : country of citizenship\n",
    "\n",
    "**(b) i94res** : country of residence\n",
    "\n",
    "**(c) i94port**: arrival airport\n",
    "\n",
    "**(d) arrdate**: arrival date.\n",
    "\n",
    "**(e) i94mode**\n",
    "\n",
    "**(f) i94addr**\n",
    "\n",
    "**(g) depdate**\n",
    "\n",
    "**(h) i94bir**\n",
    "\n",
    "**(i) i94visa**\n",
    "\n",
    "**(j) occup**\n",
    "\n",
    "**(k) biryear**\n",
    "\n",
    "**(l) dtaddto**\n",
    "\n",
    "**(m) gender**\n",
    "\n",
    "**(n) insnum**\n",
    "\n",
    "**(o) airline**\n",
    "\n",
    "**(p) admnum**\n",
    "\n",
    "**(q) fltno**\n",
    "\n",
    "**(r) visatype**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Snapshot of the Country Code Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_code = {}\n",
    "\n",
    "# Opening the SAS Label Descriptions File and processing line by line\n",
    "with open(\"data_sources/I94_SAS_Labels_Descriptions.SAS\") as sas_file:\n",
    "    for line in sas_file.readlines()[10:298]:\n",
    "        # Splitting the contents\n",
    "        pair = line.split('=')\n",
    "        # Code, country pair values\n",
    "        code, country = pair[0].strip(), pair[1].strip().strip(\"'\")\n",
    "        # storing the final results\n",
    "        country_code[code] = country\n",
    "\n",
    "# Saving the output as a .csv file for reuse\n",
    "with open(\"country_code.csv\", mode=\"w\", newline=\"\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow([\"Code\", \"Country\"])\n",
    "    for code, country in country_code.items():\n",
    "        writer.writerow([code, country])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>236</td>\n",
       "      <td>AFGHANISTAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>ALBANIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316</td>\n",
       "      <td>ALGERIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102</td>\n",
       "      <td>ANDORRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>324</td>\n",
       "      <td>ANGOLA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Code      Country\n",
       "0   236  AFGHANISTAN\n",
       "1   101      ALBANIA\n",
       "2   316      ALGERIA\n",
       "3   102      ANDORRA\n",
       "4   324       ANGOLA"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the saved .csv file\n",
    "df_country = pd.read_csv('country_code.csv')\n",
    "df_country.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Snapshot of the Airport Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the Airports Code Data in the Dataframe\n",
    "df_airports_code = pd.read_csv(\"data_sources/airport-codes_csv.csv\")\n",
    "df_airports_code.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Snapshot of the Global Land Temparature Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the Global Temparature by City Data in the Dataframe\n",
    "df_temperature = pd.read_csv(\"../../data2/GlobalLandTemperaturesByCity.csv\")\n",
    "df_temperature.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Snapshot of the the U.S. City Demographic Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the US City Demographic Data in the Dataframe\n",
    "df_demographics = pd.read_csv(\"data_sources/us-cities-demographics.csv\",delimiter=';')\n",
    "df_demographics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Snapshot of the i94port Codes and City Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>location</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ANC</td>\n",
       "      <td>ANCHORAGE</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BAR</td>\n",
       "      <td>BAKER AAF - BAKER ISLAND</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DAC</td>\n",
       "      <td>DALTONS CACHE</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PIZ</td>\n",
       "      <td>DEW STATION PT LAY DEW</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DTH</td>\n",
       "      <td>DUTCH HARBOR</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  code                  location         state\n",
       "0  ANC                 ANCHORAGE   AK         \n",
       "1  BAR  BAKER AAF - BAKER ISLAND            AK\n",
       "2  DAC             DALTONS CACHE       AK     \n",
       "3  PIZ    DEW STATION PT LAY DEW            AK\n",
       "4  DTH              DUTCH HARBOR      AK      "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the city_code dictionary\n",
    "city_code = {}\n",
    "\n",
    "# Processing line by line\n",
    "with open(\"data_sources/I94_SAS_Labels_Descriptions.SAS\") as sas_file:\n",
    "    for line in sas_file.readlines()[303:962]:\n",
    "        # Splitting the contents\n",
    "        pair = line.split('=')\n",
    "        # Code, City pair values\n",
    "        code = pair[0].strip(\"\\t\").strip().strip(\"'\")\n",
    "        city = pair[1].strip('\\t').strip().strip(\"''\")\n",
    "        # Storing the final results\n",
    "        city_code[code] = city\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "portCodes = pd.DataFrame.from_dict(city_code, orient='index', columns=['city'])\n",
    "portCodes.reset_index(inplace=True)\n",
    "portCodes.rename(columns={'index': 'code'}, inplace=True)\n",
    "\n",
    "# Function to split the city column into location and state\n",
    "def split_city(column_):\n",
    "    parts = column_.split(',')\n",
    "    if len(parts) == 2:\n",
    "        return parts\n",
    "    return [column_, None]\n",
    "\n",
    "# Apply the function to split the city column\n",
    "portCodes[['location', 'state']] = portCodes['city'].apply(split_city).apply(pd.Series)\n",
    "\n",
    "# Drop the original city column\n",
    "portCodes.drop(columns=['city'], inplace=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "portCodes.to_csv(\"city_code.csv\", index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "portCodes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame 1: (1000, 29)\n",
      "Shape of DataFrame 2: (2891, 12)\n",
      "Shape of DataFrame 3: (8599212, 7)\n",
      "Shape of DataFrame 4: (55075, 12)\n"
     ]
    }
   ],
   "source": [
    "dfs = [img_df_sample, df_demographics, df_temperature,  df_airports_code]\n",
    "# Loop through the list of DataFrames and print their shapes\n",
    "for idx, df in enumerate(dfs, 1):\n",
    "    print(f\"Shape of DataFrame {idx}: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the immigration data schema\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_spark.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>239</td>\n",
       "      <td>152592</td>\n",
       "      <td>142457</td>\n",
       "      <td>...</td>\n",
       "      <td>3095921</td>\n",
       "      <td>138429</td>\n",
       "      <td>802</td>\n",
       "      <td>477</td>\n",
       "      <td>414269</td>\n",
       "      <td>2982605</td>\n",
       "      <td>83627</td>\n",
       "      <td>0</td>\n",
       "      <td>19549</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid  i94yr  i94mon  i94cit  i94res  i94port  arrdate  i94mode  i94addr  \\\n",
       "0      0      0       0       0       0        0        0      239   152592   \n",
       "\n",
       "   depdate    ...     entdepu  matflag  biryear  dtaddto  gender   insnum  \\\n",
       "0   142457    ...     3095921   138429      802      477  414269  2982605   \n",
       "\n",
       "   airline  admnum  fltno  visatype  \n",
       "0    83627       0  19549         0  \n",
       "\n",
       "[1 rows x 28 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "# Count the number of missing values in each column\n",
    "missing_values_count = df_spark.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_spark.columns])\n",
    "\n",
    "# return outcome in pandas\n",
    "missing_values_count_ = missing_values_count.toPandas()\n",
    "missing_values_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>occup</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>insnum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99.737559</td>\n",
       "      <td>99.98734</td>\n",
       "      <td>96.327632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       occup   entdepu     insnum\n",
       "0  99.737559  99.98734  96.327632"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number of records in the DataFrame\n",
    "total_count = df_spark.count()\n",
    "\n",
    "# Count the number of missing values in each column and calculate the percentage\n",
    "missing_values_count_pd = df_spark.select([\n",
    "    ((sum(col(c).isNull().cast(\"int\")) / total_count) * 100).alias(c) for c in df_spark.columns\n",
    "]).toPandas()\n",
    "\n",
    "# Filter columns with more than 90% missing values\n",
    "deleted_columns = missing_values_count_pd.loc[:, (missing_values_count_pd > 90).any()]\n",
    "\n",
    "# Display the filtered result\n",
    "deleted_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Immigration Data Findings\n",
    "**Data Shape:** There are 3,096,313 records and 28 fields\n",
    "\n",
    "**cicid** This field is unique and can serve as a primary key for the immigration table\n",
    "\n",
    "**Duplicated Rows:** There are 0 number sof duplicated row\n",
    "\n",
    "**Columns to be Dropped:** The following columns will be dropped - 'entdepu', 'insnum' and 'occup'\n",
    "\n",
    "**Unnamed Column:** This will be dropped as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8599212 entries, 0 to 8599211\n",
      "Data columns (total 7 columns):\n",
      "dt                               object\n",
      "AverageTemperature               float64\n",
      "AverageTemperatureUncertainty    float64\n",
      "City                             object\n",
      "Country                          object\n",
      "Latitude                         object\n",
      "Longitude                        object\n",
      "dtypes: float64(2), object(5)\n",
      "memory usage: 459.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_temperature.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temperature['Country'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1743-11-01', '2013-09-01')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# min and max date\n",
    "df_temperature['dt'].min(), df_temperature['dt'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dt                                    0\n",
       "AverageTemperature               364130\n",
       "AverageTemperatureUncertainty    364130\n",
       "City                                  0\n",
       "Country                               0\n",
       "Latitude                              0\n",
       "Longitude                             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the number of missing values in each column\n",
    "missing_values_count = df_temperature.isnull().sum()\n",
    "missing_values_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dt                               0.00\n",
       "AverageTemperature               4.23\n",
       "AverageTemperatureUncertainty    4.23\n",
       "City                             0.00\n",
       "Country                          0.00\n",
       "Latitude                         0.00\n",
       "Longitude                        0.00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the total number of rows in the DataFrame\n",
    "total_rows = len(df_temperature)\n",
    "\n",
    "# Calculate the percentage of missing values in each column\n",
    "missing_values_percentage = round((missing_values_count / total_rows) * 100, 2)\n",
    "missing_values_percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temperature Data Findings\n",
    "**Other Countries** This dataset holds temperature of other countries and so we need to limit to only US.\n",
    "\n",
    "**Missing Records** There are 364130 missing records for both AverageTemperature and AverageTemperatureUncertainty which 4.23% of the total records available.\n",
    "\n",
    "**Shape** This datatset has 8,599,212 records and 7 fields.\n",
    "\n",
    "**Duplicated Rows** there are no duplicated rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airport data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 55075 entries, 0 to 55074\n",
      "Data columns (total 12 columns):\n",
      "ident           55075 non-null object\n",
      "type            55075 non-null object\n",
      "name            55075 non-null object\n",
      "elevation_ft    48069 non-null float64\n",
      "continent       27356 non-null object\n",
      "iso_country     54828 non-null object\n",
      "iso_region      55075 non-null object\n",
      "municipality    49399 non-null object\n",
      "gps_code        41030 non-null object\n",
      "iata_code       9189 non-null object\n",
      "local_code      28686 non-null object\n",
      "coordinates     55075 non-null object\n",
      "dtypes: float64(1), object(11)\n",
      "memory usage: 5.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_airports_code.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airports_code['iso_country'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airports_code.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EU    7840\n",
       "SA    7709\n",
       "AS    5350\n",
       "AF    3362\n",
       "OC    3067\n",
       "AN      28\n",
       "Name: continent, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airports_code['continent'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "continent\n",
       "AF    247\n",
       "Name: continent, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's quickly check the missing country values to see if the continent data is filled out\n",
    "df_airports_code[df_airports_code['iso_country'].isna()].groupby('continent')['continent'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>continent</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AF</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.020821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.346817</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.455681</td>\n",
       "      <td>36.823319</td>\n",
       "      <td>68.143962</td>\n",
       "      <td>98.185604</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AN</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.857143</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>96.428571</td>\n",
       "      <td>64.285714</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AS</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.411215</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.383178</td>\n",
       "      <td>51.084112</td>\n",
       "      <td>67.738318</td>\n",
       "      <td>88.411215</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EU</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.201531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.410714</td>\n",
       "      <td>50.612245</td>\n",
       "      <td>85.714286</td>\n",
       "      <td>89.400510</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OC</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.409195</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.701011</td>\n",
       "      <td>12.520378</td>\n",
       "      <td>57.091620</td>\n",
       "      <td>76.882948</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SA</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.123881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.109742</td>\n",
       "      <td>25.269166</td>\n",
       "      <td>87.183811</td>\n",
       "      <td>71.020885</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ident  type  name  elevation_ft  continent  iso_country  \\\n",
       "continent                                                            \n",
       "AF           0.0   0.0   0.0     15.020821        0.0     7.346817   \n",
       "AN           0.0   0.0   0.0      7.142857        0.0     0.000000   \n",
       "AS           0.0   0.0   0.0     36.411215        0.0     0.000000   \n",
       "EU           0.0   0.0   0.0     28.201531        0.0     0.000000   \n",
       "OC           0.0   0.0   0.0     35.409195        0.0     0.000000   \n",
       "SA           0.0   0.0   0.0      5.123881        0.0     0.000000   \n",
       "\n",
       "           iso_region  municipality   gps_code  iata_code  local_code  \\\n",
       "continent                                                               \n",
       "AF                0.0     14.455681  36.823319  68.143962   98.185604   \n",
       "AN                0.0     42.857143  14.285714  96.428571   64.285714   \n",
       "AS                0.0     15.383178  51.084112  67.738318   88.411215   \n",
       "EU                0.0     17.410714  50.612245  85.714286   89.400510   \n",
       "OC                0.0     50.701011  12.520378  57.091620   76.882948   \n",
       "SA                0.0      6.109742  25.269166  87.183811   71.020885   \n",
       "\n",
       "           coordinates  \n",
       "continent               \n",
       "AF                 0.0  \n",
       "AN                 0.0  \n",
       "AS                 0.0  \n",
       "EU                 0.0  \n",
       "OC                 0.0  \n",
       "SA                 0.0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group the DataFrame by 'continent'\n",
    "grouped = df_airports_code.groupby('continent')\n",
    "# Calculate the percentage of missing data for each group\n",
    "df_airports_missing_data_prct = grouped.apply(lambda x: x.isnull().sum() / len(x) * 100)\n",
    "df_airports_missing_data_prct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ident               0\n",
       "type                0\n",
       "name                0\n",
       "elevation_ft     7006\n",
       "continent       27719\n",
       "iso_country       247\n",
       "iso_region          0\n",
       "municipality     5676\n",
       "gps_code        14045\n",
       "iata_code       45886\n",
       "local_code      26389\n",
       "coordinates         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the number of missing values in each column\n",
    "missing_values_count = df_airports_code.isnull().sum()\n",
    "missing_values_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ident            0.00\n",
       "type             0.00\n",
       "name             0.00\n",
       "elevation_ft    12.72\n",
       "continent       50.33\n",
       "iso_country      0.45\n",
       "iso_region       0.00\n",
       "municipality    10.31\n",
       "gps_code        25.50\n",
       "iata_code       83.32\n",
       "local_code      47.91\n",
       "coordinates      0.00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the total number of rows in the DataFrame\n",
    "total_rows = len(df_airports_code)\n",
    "\n",
    "# Calculate the percentage of missing values in each column\n",
    "missing_values_percentage = round((missing_values_count / total_rows) * 100, 2)\n",
    "missing_values_percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airport Data Findings\n",
    "**ISO_COUNTRY** This 243 countries re present in the dataset.\n",
    "\n",
    "**ISO_COUNTRY FROM AF** There are 3362 records from the continent of AF, out which 247 are missing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2891 entries, 0 to 2890\n",
      "Data columns (total 12 columns):\n",
      "City                      2891 non-null object\n",
      "State                     2891 non-null object\n",
      "Median Age                2891 non-null float64\n",
      "Male Population           2888 non-null float64\n",
      "Female Population         2888 non-null float64\n",
      "Total Population          2891 non-null int64\n",
      "Number of Veterans        2878 non-null float64\n",
      "Foreign-born              2878 non-null float64\n",
      "Average Household Size    2875 non-null float64\n",
      "State Code                2891 non-null object\n",
      "Race                      2891 non-null object\n",
      "Count                     2891 non-null int64\n",
      "dtypes: float64(6), int64(2), object(4)\n",
      "memory usage: 271.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_demographics.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City                       0\n",
       "State                      0\n",
       "Median Age                 0\n",
       "Male Population            3\n",
       "Female Population          3\n",
       "Total Population           0\n",
       "Number of Veterans        13\n",
       "Foreign-born              13\n",
       "Average Household Size    16\n",
       "State Code                 0\n",
       "Race                       0\n",
       "Count                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demographics.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [City, State, Median Age, Male Population, Female Population, Total Population, Number of Veterans, Foreign-born, Average Household Size, State Code, Race, Count]\n",
       "Index: []"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the useful Primary Key Combination\n",
    "df_demographics[df_demographics[['City', 'State','Race']].duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demographics Data Findings\n",
    "**Primary Key** The combination of 'City', 'State','Race' can be used a sa joining Key. \n",
    "\n",
    "**Missing Records** There are no obvious missing records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Immgration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a view of the immigration dataset\n",
    "df_spark_ = df_spark.createOrReplaceTempView(\"us_im_table\")\n",
    "\n",
    "## SAS correspond to the number of days since 1960-01-01. Therfore, we compute the arrival dates by adding arrdate to 1960-01-01\n",
    "df_spark_ = spark.sql(\"SELECT *, date_add(to_date('1960-01-01'), arrdate) AS arrival_date FROM us_im_table\")\n",
    "df_spark_.createOrReplaceTempView(\"us_im_table\")\n",
    "\n",
    "# Replace the data in the I94VISA columns\n",
    "spark.sql(\"\"\"SELECT *, CASE \n",
    "                        WHEN i94visa = 1.0 THEN 'Business' \n",
    "                        WHEN i94visa = 2.0 THEN 'Pleasure'\n",
    "                        WHEN i94visa = 3.0 THEN 'Student'\n",
    "                        ELSE 'N/A' END AS visa_type \n",
    "                        \n",
    "                FROM us_im_table\"\"\").createOrReplaceTempView(\"us_im_table\")\n",
    "\n",
    "spark.sql(\"\"\"SELECT *, CASE \n",
    "                        WHEN depdate >= 1.0 THEN date_add(to_date('1960-01-01'), depdate)\n",
    "                        WHEN depdate IS NULL THEN NULL\n",
    "                        ELSE 'N/A' END AS departure_date \n",
    "                        \n",
    "                FROM us_im_table\"\"\").createOrReplaceTempView(\"us_im_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     375|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#.createOrReplaceTempView(\"us_im_table\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM us_im_table\n",
    "WHERE departure_date <= arrival_date\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Validating the missing numbers\n",
    "spark.sql(\"SELECT count(*) FROM us_im_table WHERE departure_date = 'N/A'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+\n",
      "|arrival_date|departure_date|\n",
      "+------------+--------------+\n",
      "|  2016-04-01|    2016-03-31|\n",
      "|  2016-04-02|    2016-03-19|\n",
      "|  2016-04-02|    2016-01-26|\n",
      "|  2016-04-02|    2016-04-01|\n",
      "|  2016-04-02|    2016-01-31|\n",
      "+------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT arrival_date, departure_date\n",
    "FROM us_im_table\n",
    "WHERE departure_date <= arrival_date\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|count(DISTINCT departure_date)|\n",
      "+------------------------------+\n",
      "|                           235|\n",
      "+------------------------------+\n",
      "\n",
      "+----------------------------+\n",
      "|count(DISTINCT arrival_date)|\n",
      "+----------------------------+\n",
      "|                          30|\n",
      "+----------------------------+\n",
      "\n",
      "+------------------------------+\n",
      "|count(DISTINCT departure_date)|\n",
      "+------------------------------+\n",
      "|                            30|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check distinct departure dates\n",
    "spark.sql(\"SELECT COUNT (DISTINCT departure_date) FROM us_im_table \").show()\n",
    "\n",
    "#check distinct arrival dates\n",
    "spark.sql(\"SELECT COUNT (DISTINCT arrival_date) FROM us_im_table \").show()\n",
    "\n",
    "#check the common values between the two sets\n",
    "spark.sql(\"\"\"   SELECT COUNT(DISTINCT departure_date) \n",
    "                FROM us_im_table \n",
    "                WHERE departure_date IN (\n",
    "                    SELECT DISTINCT arrival_date FROM us_im_table\n",
    "                ) \n",
    "                \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM us_im_table\n",
    "WHERE departure_date >= arrival_date\n",
    "\"\"\").createOrReplaceTempView(\"us_im_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|difference|count(1)|\n",
      "+----------+--------+\n",
      "|       0.0| 2953435|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT (2016-biryear)-i94bir AS difference, count(*) FROM us_im_table WHERE i94bir IS NOT NULL GROUP BY difference\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude where Gender is Missing\n",
    "spark.sql(\"\"\"SELECT * FROM us_im_table WHERE gender IN ('F', 'M')\"\"\").createOrReplaceTempView(\"us_im_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  114019|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To check the citizenship and residence data to see if any values are missing\n",
    "#citizenship countries\n",
    "spark.sql(\"\"\"\n",
    "SELECT count(*) \n",
    "FROM us_im_table\n",
    "WHERE i94cit IS NULL\n",
    "\"\"\").show()\n",
    "\n",
    "#residence countries\n",
    "spark.sql(\"\"\"\n",
    "SELECT count(*) \n",
    "FROM us_im_table\n",
    "WHERE i94res IS NULL\n",
    "\"\"\").show()\n",
    "\n",
    "#reported address\n",
    "spark.sql(\"\"\"\n",
    "SELECT count(*) \n",
    "FROM us_im_table\n",
    "WHERE i94addr IS NULL\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark_ = spark.sql(\"\"\"SELECT * FROM us_im_table\"\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cicid',\n",
       " 'i94yr',\n",
       " 'i94mon',\n",
       " 'i94cit',\n",
       " 'i94res',\n",
       " 'i94port',\n",
       " 'arrdate',\n",
       " 'i94mode',\n",
       " 'i94addr',\n",
       " 'depdate',\n",
       " 'i94bir',\n",
       " 'i94visa',\n",
       " 'count',\n",
       " 'dtadfile',\n",
       " 'visapost',\n",
       " 'occup',\n",
       " 'entdepa',\n",
       " 'entdepd',\n",
       " 'entdepu',\n",
       " 'matflag',\n",
       " 'biryear',\n",
       " 'dtaddto',\n",
       " 'gender',\n",
       " 'insnum',\n",
       " 'airline',\n",
       " 'admnum',\n",
       " 'fltno',\n",
       " 'visatype',\n",
       " 'arrival_date',\n",
       " 'visa_type',\n",
       " 'departure_date']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark_.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Our temprature datatset has got other countries, it will make sense to keep only to the United States\n",
    "df_temperature_ = df_temperature[df_temperature['Country']=='United States']\n",
    "\n",
    "# Convert the date to datetime objects\n",
    "df_temperature_['Datetime_'] = pd.to_datetime(df_temperature.dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all dates prior to 1950. The logic here is base don the fact there are no commercial air travel  prior this date.\n",
    "df_temperature_=df_temperature_[df_temperature_['Datetime_']>\"1950-01-01\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ee830a24ba8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histogram of Average Temperature values\n",
    "df_temperature_['AverageTemperature'].hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 196348 entries, 49116 to 8439246\n",
      "Data columns (total 8 columns):\n",
      "dt                               196348 non-null object\n",
      "AverageTemperature               196347 non-null float64\n",
      "AverageTemperatureUncertainty    196347 non-null float64\n",
      "City                             196348 non-null object\n",
      "Country                          196348 non-null object\n",
      "Latitude                         196348 non-null object\n",
      "Longitude                        196348 non-null object\n",
      "Datetime_                        196348 non-null datetime64[ns]\n",
      "dtypes: datetime64[ns](1), float64(2), object(5)\n",
      "memory usage: 13.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_temperature_.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning AIRPORT Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since all missing values are in africa, we simly remove them from the dataset\n",
    "df_airports_code_ = df_airports_code[df_airports_code['iso_country'].fillna('').str.upper().str.contains('US')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "excludedValues = ['closed', 'heliport', 'seaplane_base', 'balloonport']\n",
    "df_airports_code_ = df_airports_code_[~df_airports_code_['type'].str.strip().isin(excludedValues)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airports_code_ = df_airports_code_[~df_airports_code_['municipality'].isna()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airports_code_.municipality = df_airports_code_.municipality.str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply len to the iso_region field to see which ones are longer than 5 characters since the field is a combination of US and state code\n",
    "df_airports_code_['len'] = df_airports_code_[\"iso_region\"].apply(len)\n",
    "# let's remove the codes that are incorrect.\n",
    "df_airports_code_ = df_airports_code_[df_airports_code_['len']==5].copy()\n",
    "# finally, let's extract the state code\n",
    "df_airports_code_['state'] = df_airports_code_['iso_region'].str.strip().str.split(\"-\", n = 1, expand = True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ident               0\n",
       "type                0\n",
       "name                0\n",
       "elevation_ft       36\n",
       "continent       14529\n",
       "iso_country         0\n",
       "iso_region          0\n",
       "municipality        0\n",
       "gps_code          366\n",
       "iata_code       12664\n",
       "local_code        158\n",
       "coordinates         0\n",
       "len                 0\n",
       "state               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airports_code_.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demographics['City'] = df_demographics.City.str.upper().str.strip()\n",
    "# remove any leading or trailing spaces and convert to upper case\n",
    "df_demographics.City = df_demographics.City.str.strip().str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demographics['city_state_race'] = df_demographics['City'] + df_demographics['State'] + df_demographics['Race']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "      <th>city_state_race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SILVER SPRING</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "      <td>SILVER SPRINGMarylandHispanic or Latino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QUINCY</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "      <td>QUINCYMassachusettsWhite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOOVER</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "      <td>HOOVERAlabamaAsian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RANCHO CUCAMONGA</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "      <td>RANCHO CUCAMONGACaliforniaBlack or African-Ame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NEWARK</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "      <td>NEWARKNew JerseyWhite</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     SILVER SPRING       Maryland        33.8          40601.0   \n",
       "1            QUINCY  Massachusetts        41.0          44129.0   \n",
       "2            HOOVER        Alabama        38.5          38040.0   \n",
       "3  RANCHO CUCAMONGA     California        34.5          88127.0   \n",
       "4            NEWARK     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \\\n",
       "0                    2.60         MD         Hispanic or Latino  25924   \n",
       "1                    2.39         MA                      White  58723   \n",
       "2                    2.58         AL                      Asian   4759   \n",
       "3                    3.18         CA  Black or African-American  24437   \n",
       "4                    2.73         NJ                      White  76402   \n",
       "\n",
       "                                     city_state_race  \n",
       "0            SILVER SPRINGMarylandHispanic or Latino  \n",
       "1                           QUINCYMassachusettsWhite  \n",
       "2                                 HOOVERAlabamaAsian  \n",
       "3  RANCHO CUCAMONGACaliforniaBlack or African-Ame...  \n",
       "4                              NEWARKNew JerseyWhite  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demographics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City                       0\n",
       "State                      0\n",
       "Median Age                 0\n",
       "Male Population            3\n",
       "Female Population          3\n",
       "Total Population           0\n",
       "Number of Veterans        13\n",
       "Foreign-born              13\n",
       "Average Household Size    16\n",
       "State Code                 0\n",
       "Race                       0\n",
       "Count                      0\n",
       "city_state_race            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demographics.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "\n",
    "The idea here is to track the inflow of travellers into the US and as such the model is designed as a **Star schema**. In this case, the immigration data will serve as our **Fact Table**. Our **fact_immigration** table while other perspective information and features that include an object in the fact table are dimension tables. \n",
    "\n",
    "The idea of using a star schema for this task feel the most suitable for the analytics purposes of the immigration data, since it connects relevant information within a proper number of aggregations and links requiring fewer joins, and decreases the records duplication at an adequate level with respect to schema complexity.\n",
    "\n",
    "**fact_immigration** table includes the following fields :\n",
    "\n",
    "* cicid\n",
    "* citizenship_country\n",
    "* residence_country,\n",
    "* city\n",
    "* state\n",
    "* arrival_date\n",
    "* departure_date\n",
    "* age\n",
    "* visa_type\n",
    "* detailed_visa_type\n",
    "\n",
    "The dimension tables uses with four dimensions to aggregate our data:\\\n",
    "\n",
    "**dim_time :** to aggregate the data using various time units some of which are:\n",
    "\n",
    "* date\n",
    "* year\n",
    "* month\n",
    "* day\n",
    "* week\n",
    "* weekday\n",
    "* dayofyear\n",
    "\n",
    "**dim_airports :** Used to determine the areas with the largest flow of travelers. Fileds are:\n",
    "\n",
    "* ident\n",
    "* type\n",
    "* name\n",
    "* elevation_ft\n",
    "* state\n",
    "* municipality\n",
    "* iata_code\n",
    "\n",
    "**dim_city_demographics :** To look at the demographic data of the areas with the most travelers and potentially look at the impact of the flow of travellers on the demographic data (if it were updated on a regular basis). The fiels available will be:\n",
    "\n",
    "* City\n",
    "* state\n",
    "* median_age\n",
    "* male_population\n",
    "* female_population\n",
    "* total population\n",
    "* Foreign_born\n",
    "* Average_Household_Size\n",
    "* Race\n",
    "* Count\n",
    "\n",
    "**dim_temperatures :** to look at the temperature data of the cities where traveller entry and departure is being reported. The fields included will be:\n",
    "\n",
    "* date\n",
    "* City\n",
    "* average temperature\n",
    "* average temperature uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "**List the steps necessary to pipeline the data into the chosen data model**\n",
    "\n",
    "1. Clean the data (Applied in Step 2).\n",
    "1. Create Dimension tables.\n",
    "1. Create Fact table.\n",
    "1. Write data into parquet files.\n",
    "1. Perform data quality checks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demographics_spark = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \";\").load('us-cities-demographics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: string (nullable = true)\n",
      " |-- Male Population: string (nullable = true)\n",
      " |-- Female Population: string (nullable = true)\n",
      " |-- Total Population: string (nullable = true)\n",
      " |-- Number of Veterans: string (nullable = true)\n",
      " |-- Foreign-born: string (nullable = true)\n",
      " |-- Average Household Size: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demographics_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City                       object\n",
       "State                      object\n",
       "Median Age                float64\n",
       "Male Population           float64\n",
       "Female Population         float64\n",
       "Total Population            int64\n",
       "Number of Veterans        float64\n",
       "Foreign-born              float64\n",
       "Average Household Size    float64\n",
       "State Code                 object\n",
       "Race                       object\n",
       "Count                       int64\n",
       "city_state_race            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demographics.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: double (nullable = true)\n",
      " |-- Male Population: double (nullable = true)\n",
      " |-- Female Population: double (nullable = true)\n",
      " |-- Total Population: long (nullable = true)\n",
      " |-- Number of Veterans: double (nullable = true)\n",
      " |-- Foreign-born: double (nullable = true)\n",
      " |-- Average Household Size: double (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: long (nullable = true)\n",
      " |-- city_state_race: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(df_demographics).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Staging of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dictionary data\n",
    "df_countryCodes = pd.read_csv('country_code.csv')\n",
    "df_i94portCodes = pd.read_csv('city_code.csv')\n",
    "\n",
    "# load the various csv files into pandas dataframes\n",
    "df_demographics = pd.read_csv('us-cities-demographics.csv', sep=';')\n",
    "df_temperature = pd.read_csv('../../data2/GlobalLandTemperaturesByCity.csv')\n",
    "\n",
    "# load the SAS data\n",
    "df_immigration=spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_i94portCodes = pd.read_csv('city_code.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's convert the data dictionaries to views in our spark context to perform SQL operations with them\n",
    "spark_df_countryCodes = spark.createDataFrame(df_countryCodes)\n",
    "spark_df_countryCodes .createOrReplaceTempView(\"countryCodes\")\n",
    "\n",
    "# remove all entries with null values as they are either un reported or outside the US\n",
    "df_i94portCodes = df_i94portCodes[~df_i94portCodes.state.isna()].copy()\n",
    "\n",
    "# We need to exclude values for airports outside of the US. \n",
    "nonUSstates = ['CANADA', 'Canada', 'NETHERLANDS', 'NETH ANTILLES', 'THAILAND', 'ETHIOPIA', 'PRC', 'BERMUDA', 'COLOMBIA', 'ARGENTINA', 'MEXICO', \n",
    "               'BRAZIL', 'URUGUAY', 'IRELAND', 'GABON', 'BAHAMAS', 'MX', 'CAYMAN ISLAND', 'SEOUL KOREA', 'JAPAN', 'ROMANIA', 'INDONESIA',\n",
    "               'SOUTH AFRICA', 'ENGLAND', 'KENYA', 'TURK & CAIMAN', 'PANAMA', 'NEW GUINEA', 'ECUADOR', 'ITALY', 'EL SALVADOR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i94portCodes = df_i94portCodes[~df_i94portCodes.state.isin(nonUSstates)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_i94portCodes = spark.createDataFrame(df_i94portCodes)\n",
    "spark_df_i94portCodes .createOrReplaceTempView(\"i94portCodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_immigration.createOrReplaceTempView(\"immig_table\")\n",
    "# Remove all entries into the united states that weren't via air travel\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM immig_table\n",
    "WHERE i94mode = 1\n",
    "\"\"\").createOrReplaceTempView(\"immig_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows where the gender values entered is undefined\n",
    "spark.sql(\"\"\"SELECT * FROM immig_table WHERE gender IN ('F', 'M')\"\"\").createOrReplaceTempView(\"immig_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the arrival dates into a useable value\n",
    "spark.sql(\"SELECT *, date_add(to_date('1960-01-01'), arrdate) AS arrival_date FROM immig_table\").createOrReplaceTempView(\"immig_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the departure dates into a useable value\n",
    "spark.sql(\"\"\"SELECT *, CASE \n",
    "                        WHEN depdate >= 1.0 THEN date_add(to_date('1960-01-01'), depdate)\n",
    "                        WHEN depdate IS NULL THEN NULL\n",
    "                        ELSE 'N/A' END AS departure_date \n",
    "                        \n",
    "                FROM immig_table\"\"\").createOrReplaceTempView(\"immig_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#country of citizenship\n",
    "spark.sql(\"\"\"\n",
    "SELECT im.*, cc.country AS citizenship_country\n",
    "FROM immig_table im\n",
    "INNER JOIN countryCodes cc\n",
    "ON im.i94cit = cc.code\n",
    "\"\"\").createOrReplaceTempView(\"immig_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#country of residence\n",
    "spark.sql(\"\"\"\n",
    "SELECT im.*, cc.country AS residence_country\n",
    "FROM immig_table im\n",
    "INNER JOIN countryCodes cc\n",
    "ON im.i94res = cc.code\n",
    "\"\"\").createOrReplaceTempView(\"immig_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add visa character string aggregation\n",
    "spark.sql(\"\"\"SELECT *, CASE \n",
    "                        WHEN i94visa = 1.0 THEN 'Business' \n",
    "                        WHEN i94visa = 2.0 THEN 'Pleasure'\n",
    "                        WHEN i94visa = 3.0 THEN 'Student'\n",
    "                        ELSE 'N/A' END AS visa_type \n",
    "                        \n",
    "                FROM immig_table\"\"\").createOrReplaceTempView(\"immig_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add entry_port names and entry port states to the view\n",
    "spark.sql(\"\"\"\n",
    "SELECT im.*, pc.location AS entry_port, pc.state AS entry_port_state\n",
    "FROM immig_table im \n",
    "INNER JOIN i94portCodes pc\n",
    "ON im.i94port = pc.code\n",
    "\"\"\").createOrReplaceTempView(\"immig_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the age of each individual and add it to the view\n",
    "spark.sql(\"\"\"\n",
    "SELECT *, (2016-biryear) AS age \n",
    "FROM immig_table\n",
    "\"\"\").createOrReplaceTempView(\"immig_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the immigration fact data into a spark dataframe\n",
    "fact_immigration = spark.sql(\"\"\"\n",
    "                        SELECT \n",
    "                            cicid, \n",
    "                            citizenship_country,\n",
    "                            residence_country,\n",
    "                            TRIM(UPPER (entry_port)) AS city,\n",
    "                            TRIM(UPPER (entry_port_state)) AS state,\n",
    "                            arrival_date,\n",
    "                            departure_date,\n",
    "                            age,\n",
    "                            visa_type,\n",
    "                            visatype AS detailed_visa_type\n",
    "\n",
    "                        FROM immig_table\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all distinct dates from arrival and departure dates to create dimension table\n",
    "dim_time = spark.sql(\"\"\"\n",
    "SELECT DISTINCT arrival_date AS date\n",
    "FROM immig_table\n",
    "UNION\n",
    "SELECT DISTINCT departure_date AS date\n",
    "FROM immig_table\n",
    "WHERE departure_date IS NOT NULL\n",
    "\"\"\")\n",
    "dim_time.createOrReplaceTempView(\"dim_time_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_time = spark.sql(\"\"\"\n",
    "SELECT date, YEAR(date) AS year, MONTH(date) AS month, DAY(date) AS day, WEEKOFYEAR(date) AS week, DAYOFWEEK(date) as weekday, DAYOFYEAR(date) year_day\n",
    "FROM dim_time_table\n",
    "ORDER BY date ASC\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only data for the United States\n",
    "df_temperature = df_temperature[df_temperature['Country']=='United States'].copy()\n",
    "\n",
    "# Convert the date to datetime objects\n",
    "df_temperature['date'] = pd.to_datetime(df_temperature.dt)\n",
    "\n",
    "# Remove all dates prior to 1950\n",
    "df_temperature=df_temperature[df_temperature['date']>\"1950-01-01\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the city names to upper case\n",
    "df_temperature.City = df_temperature.City.str.strip().str.upper() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the dataframes from pandas to spark\n",
    "spark_df_temperature = spark.createDataFrame(df_temperature)\n",
    "spark_df_temperature .createOrReplaceTempView(\"temperature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_temperature = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    DISTINCT date, city,\n",
    "    AVG(AverageTemperature) OVER (PARTITION BY date, City) AS average_temperature, \n",
    "    AVG(AverageTemperatureUncertainty)  OVER (PARTITION BY date, City) AS average_termperature_uncertainty\n",
    "    \n",
    "FROM temperature\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demographics.City = df_demographics.City.str.strip().str.upper()\n",
    "df_demographics['State Code'] = df_demographics['State Code'].str.strip().str.upper()\n",
    "df_demographics.Race = df_demographics.Race.str.strip().str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the dataframes from pandas to spark\n",
    "spark_df_demographics = spark.createDataFrame(df_demographics)\n",
    "spark_df_demographics.createOrReplaceTempView(\"demographics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert data into the demographics dim table\n",
    "dim_demographics = spark.sql(\"\"\"\n",
    "                                SELECT  City, \n",
    "                                        State, \n",
    "                                        `Median Age` AS median_age, \n",
    "                                        `Male Population` AS male_population, \n",
    "                                        `Female Population` AS female_population, \n",
    "                                        `Total Population` AS total_population, \n",
    "                                        `Foreign-born` AS foreign_born, \n",
    "                                        `Average Household Size` AS average_household_size, \n",
    "                                        `State Code` AS state_code, \n",
    "                                        Race, \n",
    "                                        Count\n",
    "                                FROM demographics\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The airport dataset contains a lot of nulls. We'll load the csv directly into a spark dataframe to avoid having to deal with converting pandas NaN into nulls\n",
    "spark_df_airports = spark.read.format(\"csv\").option(\"header\", \"true\").load('airport-codes_csv.csv')\n",
    "spark_df_airports.createOrReplaceTempView(\"airports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#equivalent to the following pandas code:\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM airports\n",
    "WHERE iso_country IS NOT NULL\n",
    "AND UPPER(TRIM(iso_country)) LIKE 'US'\n",
    "\"\"\").createOrReplaceTempView(\"airports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM airports\n",
    "WHERE LOWER(TRIM(type)) NOT IN ('closed', 'heliport', 'seaplane_base', 'balloonport')\n",
    "AND municipality IS NOT NULL\n",
    "AND LENGTH(iso_region) = 5\n",
    "\"\"\").createOrReplaceTempView(\"airports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_airports = spark.sql(\"\"\"\n",
    "SELECT TRIM(ident) AS ident, type, name, elevation_ft, SUBSTR(iso_region, 4) AS state, TRIM(UPPER(municipality)) AS municipality, iata_code\n",
    "FROM airports\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the data in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_demographics.write.parquet(\"dim_demographics\")\n",
    "dim_time.write.parquet(\"dim_time\")\n",
    "dim_airports.write.parquet(\"dim_airports\")\n",
    "dim_temperature.write.parquet(\"dim_temperature\")\n",
    "fact_immigration.write.parquet(\"fact_immigration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "#### Running Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check some things in our data\n",
    "dim_demographics.createOrReplaceTempView(\"dim_demographics\")\n",
    "dim_time.createOrReplaceTempView(\"dim_time\")\n",
    "dim_airports.createOrReplaceTempView(\"dim_airports\")\n",
    "dim_temperature.createOrReplaceTempView(\"dim_temperature\")\n",
    "fact_immigration.createOrReplaceTempView(\"fact_immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define the following function to check for null values\n",
    "def nullValueCheck(spark_ctxt, tables_to_check):\n",
    "    \"\"\"\n",
    "    This function performs null value checks on specific columns of given tables received as parameters and raises a ValueError exception when null values are encountered.\n",
    "    It receives the following parameters:\n",
    "    spark_ctxt: spark context where the data quality check is to be performed\n",
    "    tables_to_check: A dictionary containing (table, columns) pairs specifying for each table, which column is to be checked for null values.   \n",
    "    \"\"\"  \n",
    "    for table in tables_to_check:\n",
    "        print(f\"Performing data quality check on table {table}...\")\n",
    "        for column in tables_to_check[table]:\n",
    "            returnedVal = spark_ctxt.sql(f\"\"\"SELECT COUNT(*) as nbr FROM {table} WHERE {column} IS NULL\"\"\")\n",
    "            if returnedVal.head()[0] > 0:\n",
    "                raise ValueError(f\"Data quality check failed! Found NULL values in {column} column!\")\n",
    "        print(f\"Table {table} passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing data quality check on table fact_immigration...\n",
      "Table fact_immigration passed.\n",
      "Performing data quality check on table dim_time...\n",
      "Table dim_time passed.\n",
      "Performing data quality check on table dim_demographics...\n",
      "Table dim_demographics passed.\n",
      "Performing data quality check on table dim_airports...\n",
      "Table dim_airports passed.\n",
      "Performing data quality check on table dim_temperature...\n",
      "Table dim_temperature passed.\n"
     ]
    }
   ],
   "source": [
    "#dictionary of tables and columns to be checked\n",
    "tables_to_check = { 'fact_immigration' : ['cicid'], 'dim_time':['date'], 'dim_demographics': ['City','state_code'], 'dim_airports':['ident'], 'dim_temperature':['date','City']}\n",
    "\n",
    "#We call our function on the spark context\n",
    "nullValueCheck(spark, tables_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|(count(1) - CAST(192 AS BIGINT))|\n",
      "+--------------------------------+\n",
      "|                              -7|\n",
      "+--------------------------------+\n",
      "\n",
      "+--------------------------------------------+\n",
      "|(count(DISTINCT date) - CAST(192 AS BIGINT))|\n",
      "+--------------------------------------------+\n",
      "|                                          -7|\n",
      "+--------------------------------------------+\n",
      "\n",
      "+----+\n",
      "|date|\n",
      "+----+\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#time dimension verification\n",
    "\n",
    "#check the number of rows in our time table : 192 expected\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*) - 192\n",
    "FROM dim_time\n",
    "\"\"\").show()\n",
    "\n",
    "# make sure each row has a distinct date key : 192 expected\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(DISTINCT date) - 192\n",
    "FROM dim_time\n",
    "\"\"\").show()\n",
    "\n",
    "# we could also subtract the result of one query from the other\n",
    "\n",
    "\n",
    "# and make sure all dates from the fact table are included in the time dimension (NULL is the expected result)\n",
    "spark.sql(\"\"\"\n",
    "SELECT DISTINCT date\n",
    "FROM dim_time\n",
    "\n",
    "MINUS\n",
    "\n",
    "(SELECT DISTINCT arrival_date AS date\n",
    "FROM immig_table\n",
    "UNION\n",
    "SELECT DISTINCT departure_date AS date\n",
    "FROM immig_table\n",
    "WHERE departure_date IS NOT NULL)\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+\n",
      "|(count(DISTINCT cicid) - CAST(2165257 AS BIGINT))|\n",
      "+-------------------------------------------------+\n",
      "|                                          -151553|\n",
      "+-------------------------------------------------+\n",
      "\n",
      "+-------------------------------------------------+\n",
      "|(count(DISTINCT cicid) - CAST(2165257 AS BIGINT))|\n",
      "+-------------------------------------------------+\n",
      "|                                          -151553|\n",
      "+-------------------------------------------------+\n",
      "\n",
      "+------------------------------------+\n",
      "|(count(1) - CAST(2165257 AS BIGINT))|\n",
      "+------------------------------------+\n",
      "|                             -151553|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#immigration verification\n",
    "\n",
    "# The number of primary key from the staging table (2165257 expected)\n",
    "spark.sql(\"\"\"\n",
    "SELECT count(distinct cicid) - 2165257\n",
    "FROM immig_table\n",
    "\"\"\").show()\n",
    "\n",
    "#should match the primary key count from the fact table (2165257 expected)\n",
    "spark.sql(\"\"\"\n",
    "SELECT count(distinct cicid) - 2165257\n",
    "FROM fact_immigration\n",
    "\"\"\").show()\n",
    "\n",
    "#and should match the row count from the fact table since it is also the primary key (2165257 expected)\n",
    "spark.sql(\"\"\"\n",
    "SELECT count(*) - 2165257\n",
    "FROM fact_immigration\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|(count(1) - CAST(2891 AS BIGINT))|\n",
      "+---------------------------------+\n",
      "|                                0|\n",
      "+---------------------------------+\n",
      "\n",
      "+----------------------------------------------------------+\n",
      "|(count(DISTINCT city, state, race) - CAST(2891 AS BIGINT))|\n",
      "+----------------------------------------------------------+\n",
      "|                                                         0|\n",
      "+----------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's check the demographics dimension table (2891 expected) \n",
    "spark.sql(\"\"\"\n",
    "SELECT count(*) - 2891\n",
    "FROM dim_demographics\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(DISTINCT city, state, race) - 2891\n",
    "FROM dim_demographics\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|(count(1) - CAST(14529 AS BIGINT))|\n",
      "+----------------------------------+\n",
      "|                                 0|\n",
      "+----------------------------------+\n",
      "\n",
      "+-----------------------------------------------+\n",
      "|(count(DISTINCT ident) - CAST(14529 AS BIGINT))|\n",
      "+-----------------------------------------------+\n",
      "|                                              0|\n",
      "+-----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's check the primary key for airports (expected 14529)\n",
    "spark.sql(\"\"\"\n",
    "SELECT count(*) - 14529\n",
    "FROM dim_airports\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(DISTINCT ident) - 14529\n",
    "FROM dim_airports\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|(count(1) - CAST(189472 AS BIGINT))|\n",
      "+-----------------------------------+\n",
      "|                                  0|\n",
      "+-----------------------------------+\n",
      "\n",
      "+-----------------------------------------------------+\n",
      "|(count(DISTINCT date, city) - CAST(189472 AS BIGINT))|\n",
      "+-----------------------------------------------------+\n",
      "|                                                    0|\n",
      "+-----------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#finally, city + date is our primary key for the temperature (expected 189472)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT count(*) - 189472\n",
    "FROM dim_temperature\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(DISTINCT date, city) - 189472\n",
    "FROM dim_temperature\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. fact_immigration**\n",
    "\n",
    "**Primary key:** cicid\n",
    "\n",
    "* cicid (double): Unique identifier for each traveller\n",
    "* citizenship_country (string): Traveller's country of citizenship\n",
    "* residence_country (string): Traveller's country of residence\n",
    "* city (string): City where the entry port of the traveller is located\n",
    "* state (string): State where the entry port of the traveller is located\n",
    "* arrival_date (date): Traveller's arrival date\n",
    "* departure_date (string): Traveller's departure date, if known\n",
    "* age (double):  Traveller's age\n",
    "* visa_type (string): aggregate visa type. Possible values are:\n",
    "\t\tBusiness,\n",
    "\t\tPleasure,\n",
    "\t\tStudent,\n",
    "* detailed_visa_type (string): Detailed visa types. Numerous values are available. Not all could be identified:\n",
    "\t\tB1: B1 visa is for business visits valid for up to a year\n",
    "\t\tB2: B2 visa is for pleasure visits valid for up to a year\n",
    "\t\tCP: could not find a definition\n",
    "\t\tE2: E2 investor visas allows foreign investors to enter and work inside of the United States based on a substantial investment\n",
    "\t\tF1: F1 visas are used by non-immigrant students for Academic and Language training Courses. \n",
    "\t\tF2: F2 visas are used by the dependents of F1 visa holders\n",
    "\t\tGMT: could not find a definition\n",
    "\t\tM1: for students enrolled in non-academic or “vocational study”. Mechanical, language, cooking classes, etc...\n",
    "\t\tWB: Waiver Program (WT/WB Status) travel to the United States for tourism or business for stays of 90 days or less without obtaining a visa.\n",
    "\t\tWT: Waiver Program (WT/WB Status) travel to the United States for tourism or business for stays of 90 days or less without obtaining a visa.\n",
    " \n",
    "\n",
    "\n",
    "**2. dim_demographics**\n",
    "\n",
    "**Primary key:** City, state_code, Race\n",
    "\n",
    "* City (String): Name of the city\n",
    "* State (String): Name of the state\n",
    "* median_age (double): Median age for the city\n",
    "* male_population (double): Size of the male population\n",
    "* female_population (double): Size of the female population\n",
    "* total_population (long) : Total population\n",
    "* foreign_born (double): Number of foreign born residents\n",
    "* average_household_size (double): Average size of a household\n",
    "* state_code (String): Two letter state code\n",
    "* Race (String): Racial category selected by respondants. Possible values are:\n",
    "\t\t\tHispanic or Latino\n",
    "\t\t\tWhite\n",
    "\t\t\tAsian\n",
    "\t\t\tBlack or African-American\n",
    "\t\t\tAmerican Indian and Alaska Native\n",
    "* Count (long): Number of respondants (ie Size) for the combination City, State, Race\n",
    "\n",
    "**3. dim_time**\n",
    "\n",
    "**Primary key:** date\n",
    "\n",
    "* date (string): date in the format yyyy-mm-dd\n",
    "* year (integer): year for the given date, available for aggregation\n",
    "* month (integer): month for the given date, available for aggregation\n",
    "* day (integer): day for the given date, available for aggregation\n",
    "* week (integer): week for the given date, available for aggregation (values between 0 and 52)\n",
    "* weekday (integer): weekday for the given date, available for aggregation (values between 0 and 6)\n",
    "* year_day (integer): year for the given date, available for aggregation\n",
    "\n",
    "**4. dim_airports**\n",
    "\n",
    "**Primary key:** ident\n",
    "\n",
    "* ident (string): Airport identified\n",
    "* type (string): Type of airport. Possible values are:\n",
    "\t\tlarge_airport\n",
    "\t\tmedium_airport\n",
    "\t\tsmall_airport\n",
    "* name (string): Name of the airport \n",
    "* elevation_ft (string):  Elevation of the airport\n",
    "* state (string): State where the airport is located\n",
    "* municipality (string): Name of the city closest to the airport\n",
    "* iata_code (string): iata_code that appears to airplane tickets and baggages\n",
    "\n",
    "**5. dim_temperature**\n",
    "\n",
    "**Primary key:** date, city\n",
    "\n",
    "* date (timestamp): date when the temperature was registered\n",
    "* city (string): city where the termperature was registered\n",
    "* average_temperature (double): average temperature recorded for the day in the specified city\n",
    "* average_termperature_uncertainty (double): average uncertainty recored "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "\n",
    "- Taking the volume of data into perspective the choice of is a no brainer since it over distributed processing capability for processing alrge amount of data.\n",
    "\n",
    "* Propose how often the data should be updated and why.\n",
    "\n",
    "- it would be of immense benefits if these data is refreshed on a monthly basis as this woud help the analytic steam to be able to understand seasonaity, volume of travellers on a month by month basis among other things.\n",
    "\n",
    "\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    "    - The data was increased by 100x.\n",
    "        - Even though Spark can handle the job but we would use more processing power and increase the number of nodes in the cluster. Also, we would implement the pipeline and engineering work with more sophisticated processing and storage functions such as Amazon Redshift and Amazon EMR.\n",
    "     \n",
    "    - The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "        - In this case, Apache Airflow will be used to schedule and run data pipelines, that is to ensure timeliness, consistency, and stability.\n",
    "    - The database needed to be accessed by 100+ people.\n",
    "        - In this scenario, we would move our analytics database into Amazon Redshift which can handle massive request volumes and is easily scalable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
